# Docker Image for Compression Testing
# Based on official Airflow image for 2.7.3 with Python 3.8

FROM apache/airflow:2.7.3-python3.8

# Add a unique build identifier to force different content
ARG COMPRESSION_TYPE=none
ARG BUILD_ID=1

# Layer: Application code and configuration (changes very frequently)
USER root

# Set working directory
WORKDIR /opt/airflow

# Create some test files with different content types
RUN mkdir -p /opt/airflow/test-data

# Create a large text file (good for compression)
RUN python -c "import random; import string; open('/opt/airflow/test-data/large_text.txt', 'w').write(''.join(['This is a test line with some repetitive content. ' * 10 + '\n' + 'Another line with different but still repetitive text. ' * 8 + '\n' + ''.join(random.choices(string.ascii_letters + string.digits, k=100)) + '\n' for _ in range(10000)]))"

# Create a binary-like file (less compressible)
RUN python -c "import random; open('/opt/airflow/test-data/random_data.bin', 'wb').write(bytes(random.getrandbits(8) for _ in range(50000)))"

# Create a structured data file (JSON - good for compression)
RUN python -c "import json; import random; data = [{'id': i, 'name': f'item_{i}', 'value': random.random(), 'category': random.choice(['A', 'B', 'C', 'D']), 'description': 'This is a description that repeats many times in the data structure'} for i in range(1000)]; open('/opt/airflow/test-data/structured_data.json', 'w').write(json.dumps(data, indent=2))"

# Add build metadata to make each image unique
RUN echo "Compression type: ${COMPRESSION_TYPE}" > /opt/airflow/compression-info.txt && \
    echo "Build ID: ${BUILD_ID}" >> /opt/airflow/compression-info.txt && \
    echo "Build timestamp: $(date)" >> /opt/airflow/compression-info.txt

# Copy entrypoint script (if you still want to use your custom entrypoint)
COPY dockerfiles/entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

USER airflow

EXPOSE 8080

HEALTHCHECK CMD ["curl", "-f", "http://localhost:8080/health"]

ENTRYPOINT ["/entrypoint.sh"]
CMD [] 